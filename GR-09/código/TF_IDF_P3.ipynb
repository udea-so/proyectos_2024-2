{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e60585-089e-4022-aca6-2fae4e626ae3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Set  environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24284189-fa7f-413f-99ab-da776bd2b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark package\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b2896c-13a3-4e65-aa1b-292178a41107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define  SPARK session.\n",
    "conf = SparkConf()\\\n",
    "                  .setAll([('spark.executores','5'),\n",
    "                         ('spark.executor.memory','20g'),\n",
    "                         ('spark.executor.cores','4'),\n",
    "                         ('spark.driver.memory','20g')])\\\n",
    "                  .setAppName(\"ejemplo\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b754045-cdb1-4f72-a759-e4b67d1db7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Start  SPARK Session\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d0ec5-2442-4ee2-8b28-0fbfa847fb9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Code to clean files and  find stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb136b1-7e43-453b-b6fa-c415097e446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries to be used by preprocessing process\n",
    "import nltk\n",
    "import re, string\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords =stopwords.words('english')\n",
    "stopwords[0:9]\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f0a1d2-ebc8-40af-8eb4-e0ed88a0465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You can use this expression to verify if stop words exists.\n",
    "\"ours\" in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4313bd-02b1-4925-b9e8-05ce9dda7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to delete special characters and puntuaction signs in string x\n",
    "def clean_txt(x):\n",
    "  punc = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~êëçêëåêëãêëäêëâêëâêëâêëàêëáêëáêëÜêëÖêëÑêëÉêëÇêëÅêëÄêêøêêæêêΩêêºêêªêê∫êêπêê∏êê∑êê∂êêµêê¥êê≥êê≤êê±êê∞êêØêêÆêê≠êê¨êê´êê™êê©êê®‚Öå‚Äû‚Ñ•‚Ñû‚Ñî'\n",
    "  lowercased_str = x.lower()\n",
    "  lowercased_str = lowercased_str.replace('--',' ').replace('\\\\u',' ').replace('\\\\r',' ')\n",
    "  lowercased_str = re.sub(r\"[\\\\r\\\\u!\\\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~]\",\" \",lowercased_str,flags=re.I)\n",
    "  for ch in punc:\n",
    "     lowercased_str = lowercased_str.replace(ch, '')\n",
    "  return lowercased_str.strip().encode('utf-8').decode('utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "655af6a8-a0ad-4b90-9a40-f8fbb6832fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'este texto  tiene caracteres especiales a borrar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to test the function clean_txt\n",
    "clean_txt(\"este texto# &%$ tie^ne& caracteres especia{{les~ a borrar \\n \\ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae2453-5a95-4db7-930b-a6cf1451512a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. preprocessing input files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef039a5a-50ca-4925-8cff-1b73378e7e44",
   "metadata": {},
   "source": [
    "### Step 1: Define broadcast variable to store stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "902f64e4-968e-4657-9446-8655b8e366ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# They will be send to all executors\n",
    "bc_stopwords =  sc.broadcast(stopwords)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "218789f0-afdc-4f84-ad82-e3bf469819fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the broadcast variable\n",
    "print(type(bc_stopwords.value))\n",
    "bc_stopwords.value[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c093efe-c69b-4862-ac9e-e545817ad6e5",
   "metadata": {},
   "source": [
    "### Step 2: Read input files, you can use clean_txt and additionally remove stop words\n",
    "\n",
    "Read input files, delete special characters and remove stop words. <br>\n",
    "<b>Output: </b> RDD storing the following <em>(key,value)</em> records like <em>(('doc-name', 'word'), 1)</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cad93c1c-4763-4d00-889a-f049def3fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion return a list of <key,value> pairs where key= doc-id, value=text in file\n",
    "def organize_txt(text):\n",
    "  key=text[0].split('/')[-1].split('.')[0]\n",
    "  value=text[1].split('\\n')\n",
    "  lines=[]\n",
    "  for line in value:\n",
    "    lines.append((key,line))\n",
    "  return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566492e6-5e52-4da7-879b-dc29b79cf41b",
   "metadata": {},
   "source": [
    "### guidelines\n",
    "<ul align=\"justify\">\n",
    "<li><em>wholeTextFile</em> read a directory of text files. Each file is read as a single record and returned in a <em>(key-value, pair)</em>, where the key is the path of each file, the value is the content of each file. The text files must be encoded as UTF-8.</li>\n",
    "    \n",
    "```\n",
    "OUTPUT (key, value) -->  <'path file-1',txt'>, <'path file-2',txt>, ...  <'path file-n',txt>\n",
    "```\n",
    "\n",
    "<li><em>flatMap()</em> applies the function <em>organize_txt()</em> for each line in the input RDD. As result it tags each document line with the document id. </li>\n",
    "\n",
    "```\n",
    "<'file-1',txt'>, <'file-2',txt>, ...  ) ---> ( <file-1, line-1>,.....,<file-1,line-m> ,<file-2,line-1>,<file-2,lin-2>,...)\n",
    "```\n",
    "<li><em>flatMap()</em> processes each input line from RDD and  applies word by word the  function <em>txt_clean()</em> to delete special characters and puntuaction signs. You have to define a lambda function uses every word in the text to build entries</li>\n",
    "\n",
    "```\n",
    "<file-1,line> ---> <(file-1,word-1),1>, <(file-1,word-2),1>,......., <(file-2,word-8),1>\n",
    "```\n",
    "<li><em>filter()</em>. You can use a lambda function to delete stop words in RDD</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b278a9-5e7a-4ad9-aae7-992ed5712044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('pg238', 'project'), 1),\n",
       " (('pg238', 'gutenberg'), 1),\n",
       " (('pg238', 'ebook'), 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"../Data\"\n",
    "#path=\"../Data_tmp\"\n",
    "start1 = time.time()\n",
    "words_RDD = sc.wholeTextFiles(path, minPartitions=5) \\\n",
    "                          .flatMap(lambda txt: organize_txt(txt))\\\n",
    "                          .flatMap(lambda txt:[((txt[0],clean_txt(i)),1) for i in txt[1].split()])\\\n",
    "                          .filter(lambda txt:txt[0][1]!='' and txt[0][1] not in bc_stopwords.value)\n",
    "step1_time = time.time() - start1\n",
    "words_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dd9e44a-265e-4dd2-8457-7b043533caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_RDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef60fdb-35d2-4f48-92a3-8c31da81d410",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Computing TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a073a91-77dc-4ff3-b9c6-28fdf175a97f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Compute word frequency for document, i.e. $f_{i,j}$ in expression (1)\n",
    "### guidelines\n",
    "<ul align=\"justify\">\n",
    "    <li>Use the transformation <em>reduceByKey</em> to obtain word frequency.</li>\n",
    "    \n",
    "```\n",
    "<(doc-id,word),1> , <(doc-id,word),1> -> <(doc-id,word),frequency>\n",
    "```\n",
    "    \n",
    "<li>Next, use transfomation <em> map</em> to build a new key-value pair.</li>\n",
    "    \n",
    "```\n",
    "<<(doc-id,word),frequency> -> <(doc-id, (word-1,frequency)>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f1ac087-33d5-4f66-a0fb-05a887bcb4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "start2p1 = time.time()\n",
    "word_freq=words_RDD.reduceByKey(lambda x,y:x+y)\\\n",
    "                   .map( lambda x : (x[0][0], (x[0][1],  int(x[1])) ))\n",
    "step2p1 = time.time() - start2p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9959ea4-07fb-41e1-a854-c92d8143ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pg238', ('enemy', 31)),\n",
       " ('pg238', ('wwwgutenbergorg', 5)),\n",
       " ('pg238', ('country', 20)),\n",
       " ('pg238', ('using', 6)),\n",
       " ('pg238', ('author', 1)),\n",
       " ('pg238', ('date', 3)),\n",
       " ('pg238', ('2008', 1)),\n",
       " ('pg238', ('david', 2)),\n",
       " ('pg238', ('widger', 1)),\n",
       " ('pg238', ('stone', 2))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test your computation\n",
    "word_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d673702a-55e4-4539-a969-a525227817de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define persistence for new RDD word_freq\n",
    "words_RDD.unpersist\n",
    "# ADD YOUR CODE HERE#\n",
    "word_freq.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63deea00-0329-4d96-8846-a765c8f88676",
   "metadata": {},
   "source": [
    "### Step 2: Find the max frequency among its words for each document. $MAX_k F_{i,j}$\n",
    "### Guidelines\n",
    "<li>Use transformation <em>map()</em> to iterate over RDD <em>word_freq</em> an obtain</li>\n",
    "\n",
    "```\n",
    "<(file-1,(word-1,freq-1)><(file-1,(word-2,freq-2)> --> <(file-1,freq-1)>, ...<(file-1,freq-2)>\n",
    "\n",
    "```\n",
    "<li>Use <em>groupByKey</em> and function <em>mapValues(max)</em> to obtain</li>\n",
    "\n",
    "```\n",
    "<(file-1, freq-1)>, <(file-1, freq-2)> --> <file-1, Max_freq>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e26468e-50bc-4884-8773-c07a5b18b571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pg238', 246),\n",
       " ('pg633', 83),\n",
       " ('pg464', 429),\n",
       " ('pg765', 398),\n",
       " ('pg232', 83),\n",
       " ('pg224', 793),\n",
       " ('pg529', 245),\n",
       " ('pg261', 84),\n",
       " ('pg281', 83),\n",
       " ('pg399', 524)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start2p2 = time.time()\n",
    "Max_freq = word_freq.map( lambda x : (x[0], int(x[1][1])) )\\\n",
    "                    .groupByKey().mapValues(max)\n",
    "step2p2 = time.time() - start2p2\n",
    "Max_freq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99c4bfd3-3300-4fcd-aef6-997a687f0018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Max_freq.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8adfd-fb2a-4225-9469-a7ad7f70ccec",
   "metadata": {},
   "source": [
    "### Step 3: Compute TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c9681-8412-42bf-8e27-85fb5c77120b",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "<li>Join both RDD <em>word_freq</em> and <em>Max_freq </em>  by  attribute <em>doc-id</em> and  compute TF </li>\n",
    "<li>Recall  $ TF_{ij} = \\frac{f_{ij}}{MAX_k F_{ij}} $ = frequency / max_frequency  </li>\n",
    "<li>Exchange items to build a new key-value as the following </li>\n",
    " \n",
    "\n",
    "```\n",
    "<doc-id,(word,frequency>  JOIN  <doc-id,max_frequency)>  ----> <word, (doc-i, TF) >\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "069caeee-b445-4eb2-bd89-44e5313bb0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('south', ('pg464', 0.09557109557109557)),\n",
       " ('seas', ('pg464', 0.06526806526806526)),\n",
       " ('states', ('pg464', 0.05827505827505827)),\n",
       " ('cost', ('pg464', 0.016317016317016316)),\n",
       " ('included', ('pg464', 0.011655011655011656)),\n",
       " ('located', ('pg464', 0.016317016317016316)),\n",
       " ('laws', ('pg464', 0.03263403263403263)),\n",
       " ('title', ('pg464', 0.009324009324009324)),\n",
       " ('date', ('pg464', 0.02097902097902098)),\n",
       " ('464', ('pg464', 0.002331002331002331))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start2p3 = time.time()\n",
    "TF= word_freq.join(Max_freq).map(lambda x : ( x[1][0][0],(x[0], x[1][0][1] / x[1][1] ) ))\n",
    "step2p3 = time.time() - start2p3\n",
    "step2_time = step2p1 + step2p2 + step2p3\n",
    "TF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5cb5c-4abd-40b9-8bf5-1740c6b8cc17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Computing IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707f571-8cee-4a1b-b3eb-248779592f2b",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> <font face=\"Verdana\" size='2'>\n",
    "Althought SPARK has its built-in library to compute TD-IDF,  We will build our own library.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee0a2-fab5-44d5-982a-b896b88ddbd7",
   "metadata": {},
   "source": [
    "### Step 1: For each word, compute $n_i$,  how many documents contain this one?. \n",
    "### Guidelines\n",
    "<ul align=\"justify\">\n",
    "<li>Use transformation <em>distinct()</em> to find documents where each word appears</li>\n",
    "\n",
    "```\n",
    "<(doc-id,word),1>  -> <(doc-id,word-id),1>\n",
    "```\n",
    "<li>Use transformation <em>map()</em> to build a transformed RDD </li>\n",
    "\n",
    "```\n",
    "<(doc-id,word),1> <(doc-id,word-id),1> --> <(word),1>  \n",
    "\n",
    "```\n",
    "\n",
    "    \n",
    "<li>Use transformation <em>reduceByKey()</em> to compute in how many documents does each word appear? </li>\n",
    "\n",
    "```\n",
    "<word,1>  -> <word,n_i>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20d863d-fcc5-4307-8cb1-d26f33ebbe7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clubs', 216),\n",
       " ('away', 785),\n",
       " ('nearly', 785),\n",
       " ('prevention', 62),\n",
       " ('conscientious', 159),\n",
       " ('pulpit', 174),\n",
       " ('personable', 16),\n",
       " ('ugly', 462),\n",
       " ('lime', 127),\n",
       " ('brazen-throated', 4),\n",
       " ('cigar', 247),\n",
       " ('hath', 281),\n",
       " ('therefore', 603),\n",
       " ('companion', 514),\n",
       " ('second', 785)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start3p1 = time.time()\n",
    "occ = words_RDD.distinct()\\\n",
    "               .map( lambda x : ((x[0][1], 1))) \\\n",
    "               .reduceByKey(lambda x,y : x+y )\n",
    "step3p1 = time.time() - start3p1\n",
    "occ.take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9ad3a-d44b-4457-a858-70868a00bdf1",
   "metadata": {},
   "source": [
    "### Step 2: Compute IDF for each word. \n",
    "### Guidelines\n",
    "<li> Find number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d09e0f0-ea3e-4d80-adc5-c3a1215cba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start3p2 = time.time()\n",
    "nro_docs=len(sc.wholeTextFiles(\"../Data/\", minPartitions=3)\\\n",
    "               .map(lambda txt: txt[0].split('/')[-1].split('.')[0]).collect())\n",
    "step3p2 = time.time() - start3p2\n",
    "nro_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4684f-ef8c-4da1-aecd-fff4925bdc78",
   "metadata": {},
   "source": [
    "<li> Compute $IDF_i = \\log_2 \\frac {N}{n_i}\\tag{2}$  </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52dff04a-a652-4c45-9e68-64e84c97228c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clubs', 0.5604159055943216),\n",
       " ('away', 0.0),\n",
       " ('nearly', 0.0),\n",
       " ('prevention', 1.1024779672469986),\n",
       " ('conscientious', 0.693472532424801),\n",
       " ('pulpit', 0.6543204084626528),\n",
       " ('personable', 1.6907496740893277),\n",
       " ('ugly', 0.23022768118912704),\n",
       " ('lime', 0.7910659357892956),\n",
       " ('brazen-throated', 2.2928096654172903)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import *\n",
    "start3p3 = time.time()\n",
    "IDF=occ.map(lambda w: (w[0],math.log10(nro_docs/w[1])))\n",
    "step3p3 = time.time() - start3p3\n",
    "step3_time = step3p1 + step3p2 + step3p3\n",
    "IDF.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459cfb6-46a9-404d-838f-ab29d3c1b478",
   "metadata": {},
   "source": [
    "## 6. Computing TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822fd41-2f6b-4454-a818-86f6adcad1c9",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "<li> We have two RDD.</li>   \n",
    "\n",
    "```\n",
    " TF: <word, (doc-id,TF)>  and  IDF: <word, IDF>\n",
    "```\n",
    " <li>Use the join transformation to build the transformed RDD  </li>\n",
    "\n",
    "```\n",
    " TF_IDF: <word, (doc-id,TF * IDF)>\n",
    "```\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb82a16f-f286-4617-acd8-d7a4a7ec0619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pg464', ('cost', 0.016317016317016316, 0.0, 0.0)),\n",
       " ('pg232', ('cost', 0.04819277108433735, 0.0, 0.0)),\n",
       " ('pg224', ('cost', 0.007566204287515763, 0.0, 0.0)),\n",
       " ('pg529', ('cost', 0.0163265306122449, 0.0, 0.0)),\n",
       " ('pg399', ('cost', 0.01717557251908397, 0.0, 0.0))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start4 = time.time()\n",
    "tf_idf=TF.join(IDF).map(lambda x: (x[1][0][0],(x[0],x[1][0][1],x[1][1],x[1][0][1]*x[1][1])))\n",
    "step4_time = time.time() - start4\n",
    "tf_idf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4858b6ea-d3b6-4d8e-96ac-ad4f0ee718da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------------+------------------+------------------+\n",
      "|DocumentId|        Word|                TF|               IDF|            TF_IDF|\n",
      "+----------+------------+------------------+------------------+------------------+\n",
      "|     pg432|    strether|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg208|winterbourne|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg347|     grettir|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg338|      iktomi|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg500|   pinocchio|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg212|       00000|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg268|    annixter|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg401|       condy|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg823|      declan|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg716|    cleggett|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg125|      elnora|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg491|     rezanov|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg806| philoktetes|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg267|    glennard|               1.0|2.5938396610812715|2.5938396610812715|\n",
      "|     pg532|      dannie| 0.990521327014218|2.5938396610812715|2.5692535031563306|\n",
      "|     pg165|    mcteague|0.9609483960948396|2.5938396610812715|  2.49254606204323|\n",
      "|     pg391|         aoi|0.9553072625698324|2.5938396610812715|2.4779138661726114|\n",
      "|     pg806| neoptolemos|0.9385474860335196|2.5938396610812715| 2.434441693081864|\n",
      "|     pg673|        tttt|               1.0|  2.41774840202559|  2.41774840202559|\n",
      "|     pg709|      curdie|               1.0|  2.41774840202559|  2.41774840202559|\n",
      "+----------+------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to display all values\n",
    "df_tf_idf=tf_idf.map(lambda x: (x[0],x[1][0],x[1][1],x[1][2],x[1][3])).toDF([\"DocumentId\",\"Word\",\"TF\",\"IDF\",\"TF_IDF\"])\n",
    "df_tf_idf.orderBy('TF_IDF',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "014d4c25-37eb-4afd-a848-d96c5083554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_time = step1_time + step2_time + step3_time + step4_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b681e69b-407e-44df-8b72-76b9bfd2ba40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Metrics:\n",
      "1. Text Cleaning and Loading: 0.79 seconds\n",
      "2. TF Calculation: 0.17 seconds\n",
      "3. IDF Calculation: 21.11 seconds\n",
      "4. TF-IDF and Keyword Extraction: 0.04 seconds\n",
      "Overall Time: 22.12 seconds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Timing Metrics:\")\n",
    "print(f\"1. Text Cleaning and Loading: {step1_time:.2f} seconds\")\n",
    "print(f\"2. TF Calculation: {step2_time:.2f} seconds\")\n",
    "print(f\"3. IDF Calculation: {step3_time:.2f} seconds\")\n",
    "print(f\"4. TF-IDF and Keyword Extraction: {step4_time:.2f} seconds\")\n",
    "print(f\"Overall Time: {overall_time:.2f} seconds\")\n",
    "print(f\"\\n\")\n",
    "print(f\"\\n\")\n",
    "print(f\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
